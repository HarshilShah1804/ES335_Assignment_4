{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the CPU\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Running on the GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Running on the CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformation = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "horse_train = []\n",
    "panda_train = []\n",
    "horse_test = []\n",
    "panda_test = []\n",
    "for filename in os.listdir(\"images/Horse_train\"):\n",
    "    img = torchvision.io.read_image(\"images/Horse_train/\"+filename)\n",
    "    img = img.float() / 255.0\n",
    "    horse_train.append(img)\n",
    "for filename in os.listdir(\"images/Panda_train\"):\n",
    "    img = torchvision.io.read_image(\"images/Panda_train/\"+filename)\n",
    "    img = img.float() / 255.0\n",
    "    panda_train.append(img)\n",
    "for filename in os.listdir(\"images/Horse_test\"):\n",
    "    img = torchvision.io.read_image(\"images/Horse_test/\"+filename)\n",
    "    img = img.float() / 255.0\n",
    "    horse_test.append(img)\n",
    "for filename in os.listdir(\"images/Panda_test\"):\n",
    "    img = torchvision.io.read_image(\"images/Panda_test/\"+filename)\n",
    "    img = img.float() / 255.0\n",
    "    panda_test.append(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train:  torch.Size([160, 3, 180, 180])\n",
      "Shape of y_train:  torch.Size([160])\n",
      "Shape of X_test:  torch.Size([40, 3, 180, 180])\n",
      "Shape of y_test:  torch.Size([40])\n"
     ]
    }
   ],
   "source": [
    "X_train = torch.cat((torch.stack(horse_train), torch.stack(panda_train)))\n",
    "y_train = torch.cat((torch.zeros(len(horse_train)), torch.ones(len(panda_train))))  # Class 0 for horse, class 1 for panda\n",
    "X_test = torch.cat((torch.stack(horse_test), torch.stack(panda_test)))\n",
    "y_test = torch.cat((torch.zeros(len(horse_test)), torch.ones(len(panda_test)))) # Class 0 for horse, class 1 for panda\n",
    "\n",
    "train_indices = torch.randperm(len(X_train))\n",
    "X_train = X_train[train_indices]\n",
    "y_train = y_train[train_indices]\n",
    "\n",
    "test_indices = torch.randperm(len(X_test))\n",
    "X_test = X_test[test_indices]\n",
    "y_test = y_test[test_indices]\n",
    "\n",
    "X_train = X_train.to(device)\n",
    "y_train = y_train.to(device)\n",
    "X_test = X_test.to(device)\n",
    "y_test = y_test.to(device)\n",
    "\n",
    "print(\"Shape of X_train: \", X_train.shape)\n",
    "print(\"Shape of y_train: \", y_train.shape)\n",
    "print(\"Shape of X_test: \", X_test.shape)\n",
    "print(\"Shape of y_test: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0.,\n",
      "        0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0.,\n",
      "        0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0.,\n",
      "        0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0.,\n",
      "        0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0.,\n",
      "        1., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "print(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG (1 Block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vgg_1_block(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "  (relu): ReLU()\n",
      "  (maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (fc1): Linear(in_features=259200, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "Number of parameters:  33178753\n",
      "Number of trainable parameters:  33178753\n"
     ]
    }
   ],
   "source": [
    "class vgg_1_block(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(vgg_1_block, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels = 3, out_channels = 32, kernel_size = 3, padding='same')\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(32*90*90, 128)\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "    \n",
    "vgg1b = vgg_1_block().to(device)\n",
    "print(vgg1b)\n",
    "print(\"Number of parameters: \", sum(p.numel() for p in vgg1b.parameters()))\n",
    "print(\"Number of trainable parameters: \", sum(p.numel() for p in vgg1b.parameters() if p.requires_grad))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 0.6917229890823364\n",
      "Epoch: 1 Loss: 0.6881644129753113\n",
      "Epoch: 2 Loss: 0.6831415295600891\n",
      "Epoch: 3 Loss: 0.677044689655304\n",
      "Epoch: 4 Loss: 0.6698112487792969\n",
      "Epoch: 5 Loss: 0.6615290641784668\n",
      "Epoch: 6 Loss: 0.6521018147468567\n",
      "Epoch: 7 Loss: 0.6418759822845459\n",
      "Epoch: 8 Loss: 0.6309963464736938\n",
      "Epoch: 9 Loss: 0.619530439376831\n",
      "Epoch: 10 Loss: 0.6077923774719238\n",
      "Epoch: 11 Loss: 0.5958319306373596\n",
      "Epoch: 12 Loss: 0.5838675498962402\n",
      "Epoch: 13 Loss: 0.5722123384475708\n",
      "Epoch: 14 Loss: 0.5608342885971069\n",
      "Epoch: 15 Loss: 0.5496755838394165\n",
      "Epoch: 16 Loss: 0.5388544797897339\n",
      "Epoch: 17 Loss: 0.5283926129341125\n",
      "Epoch: 18 Loss: 0.5182187557220459\n",
      "Epoch: 19 Loss: 0.5084472298622131\n",
      "Epoch: 20 Loss: 0.4990082383155823\n",
      "Epoch: 21 Loss: 0.48986977338790894\n",
      "Epoch: 22 Loss: 0.48101434111595154\n",
      "Epoch: 23 Loss: 0.4724217355251312\n",
      "Epoch: 24 Loss: 0.4640778601169586\n",
      "Epoch: 25 Loss: 0.4559554159641266\n",
      "Epoch: 26 Loss: 0.4480156898498535\n",
      "Epoch: 27 Loss: 0.4402875304222107\n",
      "Epoch: 28 Loss: 0.43273982405662537\n",
      "Epoch: 29 Loss: 0.42534342408180237\n",
      "Epoch: 30 Loss: 0.4180925786495209\n",
      "Epoch: 31 Loss: 0.4110003411769867\n",
      "Epoch: 32 Loss: 0.40406718850135803\n",
      "Epoch: 33 Loss: 0.39727073907852173\n",
      "Epoch: 34 Loss: 0.3906109929084778\n",
      "Epoch: 35 Loss: 0.3841167092323303\n",
      "Epoch: 36 Loss: 0.37777549028396606\n",
      "Epoch: 37 Loss: 0.37155473232269287\n",
      "Epoch: 38 Loss: 0.3654686510562897\n",
      "Epoch: 39 Loss: 0.35952311754226685\n",
      "Epoch: 40 Loss: 0.3537197709083557\n",
      "Epoch: 41 Loss: 0.34806329011917114\n",
      "Epoch: 42 Loss: 0.3425200581550598\n",
      "Epoch: 43 Loss: 0.33708640933036804\n",
      "Epoch: 44 Loss: 0.3317766785621643\n",
      "Epoch: 45 Loss: 0.3265645503997803\n",
      "Epoch: 46 Loss: 0.32146960496902466\n",
      "Epoch: 47 Loss: 0.3164457678794861\n",
      "Epoch: 48 Loss: 0.3114926815032959\n",
      "Epoch: 49 Loss: 0.3066219389438629\n",
      "Epoch: 50 Loss: 0.30181780457496643\n",
      "Epoch: 51 Loss: 0.2970767915248871\n",
      "Epoch: 52 Loss: 0.2923962473869324\n",
      "Epoch: 53 Loss: 0.28776267170906067\n",
      "Epoch: 54 Loss: 0.28317752480506897\n",
      "Epoch: 55 Loss: 0.27864107489585876\n",
      "Epoch: 56 Loss: 0.2741660177707672\n",
      "Epoch: 57 Loss: 0.2697211802005768\n",
      "Epoch: 58 Loss: 0.2653297781944275\n",
      "Epoch: 59 Loss: 0.26099392771720886\n",
      "Epoch: 60 Loss: 0.2566995322704315\n",
      "Epoch: 61 Loss: 0.2524431347846985\n",
      "Epoch: 62 Loss: 0.24825438857078552\n",
      "Epoch: 63 Loss: 0.2440977841615677\n",
      "Epoch: 64 Loss: 0.23999691009521484\n",
      "Epoch: 65 Loss: 0.23594415187835693\n",
      "Epoch: 66 Loss: 0.23193767666816711\n",
      "Epoch: 67 Loss: 0.22797377407550812\n",
      "Epoch: 68 Loss: 0.2240593135356903\n",
      "Epoch: 69 Loss: 0.22019371390342712\n",
      "Epoch: 70 Loss: 0.21638111770153046\n",
      "Epoch: 71 Loss: 0.21261398494243622\n",
      "Epoch: 72 Loss: 0.20889563858509064\n",
      "Epoch: 73 Loss: 0.20521926879882812\n",
      "Epoch: 74 Loss: 0.20158982276916504\n",
      "Epoch: 75 Loss: 0.1980169713497162\n",
      "Epoch: 76 Loss: 0.19449014961719513\n",
      "Epoch: 77 Loss: 0.19100359082221985\n",
      "Epoch: 78 Loss: 0.18756535649299622\n",
      "Epoch: 79 Loss: 0.18417243659496307\n",
      "Epoch: 80 Loss: 0.1808263212442398\n",
      "Epoch: 81 Loss: 0.177527517080307\n",
      "Epoch: 82 Loss: 0.17428052425384521\n",
      "Epoch: 83 Loss: 0.17107358574867249\n",
      "Epoch: 84 Loss: 0.16792012751102448\n",
      "Epoch: 85 Loss: 0.16481083631515503\n",
      "Epoch: 86 Loss: 0.16174545884132385\n",
      "Epoch: 87 Loss: 0.15872803330421448\n",
      "Epoch: 88 Loss: 0.1557636559009552\n",
      "Epoch: 89 Loss: 0.1528405398130417\n",
      "Epoch: 90 Loss: 0.14996378123760223\n",
      "Epoch: 91 Loss: 0.14713959395885468\n",
      "Epoch: 92 Loss: 0.1443643867969513\n",
      "Epoch: 93 Loss: 0.14162778854370117\n",
      "Epoch: 94 Loss: 0.13894590735435486\n",
      "Epoch: 95 Loss: 0.13629642128944397\n",
      "Epoch: 96 Loss: 0.13370896875858307\n",
      "Epoch: 97 Loss: 0.13116607069969177\n",
      "Epoch: 98 Loss: 0.12866024672985077\n",
      "Epoch: 99 Loss: 0.12621033191680908\n",
      "Epoch: 100 Loss: 0.1237974539399147\n",
      "Epoch: 101 Loss: 0.12143216282129288\n",
      "Epoch: 102 Loss: 0.11911487579345703\n",
      "Epoch: 103 Loss: 0.11684130132198334\n",
      "Epoch: 104 Loss: 0.11461533606052399\n",
      "Epoch: 105 Loss: 0.11242114007472992\n",
      "Epoch: 106 Loss: 0.11028355360031128\n",
      "Epoch: 107 Loss: 0.10818526893854141\n",
      "Epoch: 108 Loss: 0.10612400621175766\n",
      "Epoch: 109 Loss: 0.10410626232624054\n",
      "Epoch: 110 Loss: 0.10213259607553482\n",
      "Epoch: 111 Loss: 0.1001976951956749\n",
      "Epoch: 112 Loss: 0.09830393642187119\n",
      "Epoch: 113 Loss: 0.09644939005374908\n",
      "Epoch: 114 Loss: 0.0946321114897728\n",
      "Epoch: 115 Loss: 0.09285945445299149\n",
      "Epoch: 116 Loss: 0.09111957997083664\n",
      "Epoch: 117 Loss: 0.08941961824893951\n",
      "Epoch: 118 Loss: 0.08775679767131805\n",
      "Epoch: 119 Loss: 0.08612720668315887\n",
      "Epoch: 120 Loss: 0.08453185111284256\n",
      "Epoch: 121 Loss: 0.08297698199748993\n",
      "Epoch: 122 Loss: 0.08145115524530411\n",
      "Epoch: 123 Loss: 0.07995963096618652\n",
      "Epoch: 124 Loss: 0.07850505411624908\n",
      "Epoch: 125 Loss: 0.07708006352186203\n",
      "Epoch: 126 Loss: 0.07568572461605072\n",
      "Epoch: 127 Loss: 0.07432547956705093\n",
      "Epoch: 128 Loss: 0.07299300283193588\n",
      "Epoch: 129 Loss: 0.07169176638126373\n",
      "Epoch: 130 Loss: 0.0704188272356987\n",
      "Epoch: 131 Loss: 0.06917321681976318\n",
      "Epoch: 132 Loss: 0.06795863062143326\n",
      "Epoch: 133 Loss: 0.06676845997571945\n",
      "Epoch: 134 Loss: 0.06560634076595306\n",
      "Epoch: 135 Loss: 0.06447140872478485\n",
      "Epoch: 136 Loss: 0.06335972249507904\n",
      "Epoch: 137 Loss: 0.06227456405758858\n",
      "Epoch: 138 Loss: 0.061213623732328415\n",
      "Epoch: 139 Loss: 0.06017399951815605\n",
      "Epoch: 140 Loss: 0.059160999953746796\n",
      "Epoch: 141 Loss: 0.05816922336816788\n",
      "Epoch: 142 Loss: 0.057200245559215546\n",
      "Epoch: 143 Loss: 0.05625415965914726\n",
      "Epoch: 144 Loss: 0.055326759815216064\n",
      "Epoch: 145 Loss: 0.05442105978727341\n",
      "Epoch: 146 Loss: 0.053536634892225266\n",
      "Epoch: 147 Loss: 0.05267059803009033\n",
      "Epoch: 148 Loss: 0.051823507994413376\n",
      "Epoch: 149 Loss: 0.0509960874915123\n",
      "Epoch: 150 Loss: 0.050188176333904266\n",
      "Epoch: 151 Loss: 0.049396585673093796\n",
      "Epoch: 152 Loss: 0.048624493181705475\n",
      "Epoch: 153 Loss: 0.04786808416247368\n",
      "Epoch: 154 Loss: 0.04712839797139168\n",
      "Epoch: 155 Loss: 0.04640353471040726\n",
      "Epoch: 156 Loss: 0.045698050409555435\n",
      "Epoch: 157 Loss: 0.045004140585660934\n",
      "Epoch: 158 Loss: 0.04432637244462967\n",
      "Epoch: 159 Loss: 0.043664418160915375\n",
      "Epoch: 160 Loss: 0.043015748262405396\n",
      "Epoch: 161 Loss: 0.0423801951110363\n",
      "Epoch: 162 Loss: 0.04176155850291252\n",
      "Epoch: 163 Loss: 0.04115178436040878\n",
      "Epoch: 164 Loss: 0.040557265281677246\n",
      "Epoch: 165 Loss: 0.03997771814465523\n",
      "Epoch: 166 Loss: 0.03940577059984207\n",
      "Epoch: 167 Loss: 0.03884824365377426\n",
      "Epoch: 168 Loss: 0.03830305486917496\n",
      "Epoch: 169 Loss: 0.03776692599058151\n",
      "Epoch: 170 Loss: 0.03724292665719986\n",
      "Epoch: 171 Loss: 0.03673065826296806\n",
      "Epoch: 172 Loss: 0.036227934062480927\n",
      "Epoch: 173 Loss: 0.03573668748140335\n",
      "Epoch: 174 Loss: 0.0352540984749794\n",
      "Epoch: 175 Loss: 0.0347818061709404\n",
      "Epoch: 176 Loss: 0.034318555146455765\n",
      "Epoch: 177 Loss: 0.03386427462100983\n",
      "Epoch: 178 Loss: 0.033420953899621964\n",
      "Epoch: 179 Loss: 0.03298446536064148\n",
      "Epoch: 180 Loss: 0.03255746513605118\n",
      "Epoch: 181 Loss: 0.032138992100954056\n",
      "Epoch: 182 Loss: 0.03172868490219116\n",
      "Epoch: 183 Loss: 0.03132673725485802\n",
      "Epoch: 184 Loss: 0.030933286994695663\n",
      "Epoch: 185 Loss: 0.03054606355726719\n",
      "Epoch: 186 Loss: 0.030165156349539757\n",
      "Epoch: 187 Loss: 0.02979334257543087\n",
      "Epoch: 188 Loss: 0.029428714886307716\n",
      "Epoch: 189 Loss: 0.02906980738043785\n",
      "Epoch: 190 Loss: 0.02871883288025856\n",
      "Epoch: 191 Loss: 0.02837356925010681\n",
      "Epoch: 192 Loss: 0.028035104274749756\n",
      "Epoch: 193 Loss: 0.027703726664185524\n",
      "Epoch: 194 Loss: 0.02737775444984436\n",
      "Epoch: 195 Loss: 0.027057060971856117\n",
      "Epoch: 196 Loss: 0.026743143796920776\n",
      "Epoch: 197 Loss: 0.026434043422341347\n",
      "Epoch: 198 Loss: 0.026131117716431618\n",
      "Epoch: 199 Loss: 0.02583395503461361\n",
      "Epoch: 200 Loss: 0.025541294366121292\n",
      "Epoch: 201 Loss: 0.02525457739830017\n",
      "Epoch: 202 Loss: 0.024973196908831596\n",
      "Epoch: 203 Loss: 0.024696385487914085\n",
      "Epoch: 204 Loss: 0.024423908442258835\n",
      "Epoch: 205 Loss: 0.024156605824828148\n",
      "Epoch: 206 Loss: 0.023894090205430984\n",
      "Epoch: 207 Loss: 0.023635882884263992\n",
      "Epoch: 208 Loss: 0.0233822800219059\n",
      "Epoch: 209 Loss: 0.02313326857984066\n",
      "Epoch: 210 Loss: 0.02288782224059105\n",
      "Epoch: 211 Loss: 0.022647034376859665\n",
      "Epoch: 212 Loss: 0.02241010218858719\n",
      "Epoch: 213 Loss: 0.02217765524983406\n",
      "Epoch: 214 Loss: 0.021948400884866714\n",
      "Epoch: 215 Loss: 0.021723151206970215\n",
      "Epoch: 216 Loss: 0.02150239795446396\n",
      "Epoch: 217 Loss: 0.021283844485878944\n",
      "Epoch: 218 Loss: 0.021069299429655075\n",
      "Epoch: 219 Loss: 0.02085886336863041\n",
      "Epoch: 220 Loss: 0.02065088227391243\n",
      "Epoch: 221 Loss: 0.020447267219424248\n",
      "Epoch: 222 Loss: 0.02024655044078827\n",
      "Epoch: 223 Loss: 0.020048748701810837\n",
      "Epoch: 224 Loss: 0.019853876903653145\n",
      "Epoch: 225 Loss: 0.01966303028166294\n",
      "Epoch: 226 Loss: 0.019474107772111893\n",
      "Epoch: 227 Loss: 0.019288863986730576\n",
      "Epoch: 228 Loss: 0.019106250256299973\n",
      "Epoch: 229 Loss: 0.018926212564110756\n",
      "Epoch: 230 Loss: 0.018748871982097626\n",
      "Epoch: 231 Loss: 0.01857546530663967\n",
      "Epoch: 232 Loss: 0.018403273075819016\n",
      "Epoch: 233 Loss: 0.01823379099369049\n",
      "Epoch: 234 Loss: 0.018067892640829086\n",
      "Epoch: 235 Loss: 0.01790355145931244\n",
      "Epoch: 236 Loss: 0.017741311341524124\n",
      "Epoch: 237 Loss: 0.017581786960363388\n",
      "Epoch: 238 Loss: 0.017425360158085823\n",
      "Epoch: 239 Loss: 0.017269933596253395\n",
      "Epoch: 240 Loss: 0.017117414623498917\n",
      "Epoch: 241 Loss: 0.01696750707924366\n",
      "Epoch: 242 Loss: 0.016819220036268234\n",
      "Epoch: 243 Loss: 0.016672790050506592\n",
      "Epoch: 244 Loss: 0.016529276967048645\n",
      "Epoch: 245 Loss: 0.016387440264225006\n",
      "Epoch: 246 Loss: 0.016247307881712914\n",
      "Epoch: 247 Loss: 0.016109246760606766\n",
      "Epoch: 248 Loss: 0.01597370207309723\n",
      "Epoch: 249 Loss: 0.015839388594031334\n",
      "Epoch: 250 Loss: 0.015707096084952354\n",
      "Epoch: 251 Loss: 0.01557681430131197\n",
      "Epoch: 252 Loss: 0.015448194928467274\n",
      "Epoch: 253 Loss: 0.015321239829063416\n",
      "Epoch: 254 Loss: 0.015196211636066437\n",
      "Epoch: 255 Loss: 0.01507278997451067\n",
      "Epoch: 256 Loss: 0.014950722455978394\n",
      "Epoch: 257 Loss: 0.014830837957561016\n",
      "Epoch: 258 Loss: 0.014712156727910042\n",
      "Epoch: 259 Loss: 0.01459494512528181\n",
      "Epoch: 260 Loss: 0.014479616656899452\n",
      "Epoch: 261 Loss: 0.014365704730153084\n",
      "Epoch: 262 Loss: 0.014253201894462109\n",
      "Epoch: 263 Loss: 0.014142165891826153\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m----> 5\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m vgg1b(X_train)\n\u001b[0;32m      7\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(y_pred, y_train\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\Harshil Shah\\python310\\Lib\\site-packages\\torch\\_compile.py:31\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     28\u001b[0m     disable_fn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mdisable(fn, recursive)\n\u001b[0;32m     29\u001b[0m     fn\u001b[38;5;241m.\u001b[39m__dynamo_disable \u001b[38;5;241m=\u001b[39m disable_fn\n\u001b[1;32m---> 31\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdisable_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Harshil Shah\\python310\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:600\u001b[0m, in \u001b[0;36mDisableContext.__call__.<locals>._fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    598\u001b[0m prior \u001b[38;5;241m=\u001b[39m set_eval_frame(callback)\n\u001b[0;32m    599\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 600\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    601\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    602\u001b[0m     set_eval_frame(prior)\n",
      "File \u001b[1;32mc:\\Users\\Harshil Shah\\python310\\Lib\\site-packages\\torch\\optim\\optimizer.py:947\u001b[0m, in \u001b[0;36mOptimizer.zero_grad\u001b[1;34m(self, set_to_none)\u001b[0m\n\u001b[0;32m    945\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    946\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m set_to_none:\n\u001b[1;32m--> 947\u001b[0m         p\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    948\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mgrad_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(vgg1b.parameters(), lr=0.001, momentum=0.9)\n",
    "epochs = 1000\n",
    "for i in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = vgg1b(X_train)\n",
    "    loss = criterion(y_pred, y_train.unsqueeze(1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(\"Epoch:\", i, \"Loss:\", loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set:  0.699999988079071\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    y_pred = vgg1b(X_test)\n",
    "    y_pred = (y_pred > 0.5).float()\n",
    "    accuracy = (y_pred == y_test.unsqueeze(1)).float().mean()\n",
    "    print(\"Accuracy on test set: \", accuracy.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
